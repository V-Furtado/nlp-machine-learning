{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwKpigCY3XFb"
      },
      "source": [
        "#  NLP: \"Fake News Detection\"\n",
        "\n",
        "Objective: Build a classifier that can distinguish real news from fake news. A training set for this is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jG2i7iU3kD7"
      },
      "source": [
        "#Action items for this exercise:\n",
        "\n",
        "* Use the vector model and text preprocessing techniques from class to construct a training data set.\n",
        "* Determine the dimensions of your vector model and print out the first 10 dimensions\n",
        "* Use that training data set to construct a Naive Bayes classifier.\n",
        "* Compute the accuracy and 95% CI for the classifier.\n",
        "* Try your analysis with and without data preprocessing, is there a difference in accuracies of the models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZC9I9a84qmT"
      },
      "source": [
        "Note: The data set contains a large number of articles (takes a long time to train), you can downsample this to something like 1,000 articles or so in order to speed up training and evaluation.\n",
        "\n",
        "The fields you are interested in are ‘text’ and ‘label’ with the obvious interpretations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3XADNJ2G3Q4P",
        "outputId": "9ecefe11-c4c2-4dcd-f9c0-436027d93ac0"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmNb76oJ5wsC"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/lutzhamel/fake-news/master/data/fake_or_real_news.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "6IqzxTIk59sm",
        "outputId": "94f74495-77ff-44f5-d979-ab773044c7c2"
      },
      "source": [
        "df = pd.read_csv(url)\n",
        "fetch_20newsgroups = df \n",
        "fetch_20newsgroups"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6330</th>\n",
              "      <td>4490</td>\n",
              "      <td>State Department says it can't find emails fro...</td>\n",
              "      <td>The State Department told the Republican Natio...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6331</th>\n",
              "      <td>8062</td>\n",
              "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
              "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6332</th>\n",
              "      <td>8622</td>\n",
              "      <td>Anti-Trump Protesters Are Tools of the Oligarc...</td>\n",
              "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6333</th>\n",
              "      <td>4021</td>\n",
              "      <td>In Ethiopia, Obama seeks progress on peace, se...</td>\n",
              "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6334</th>\n",
              "      <td>4330</td>\n",
              "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
              "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6335 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... label\n",
              "0      8476  ...  FAKE\n",
              "1     10294  ...  FAKE\n",
              "2      3608  ...  REAL\n",
              "3     10142  ...  FAKE\n",
              "4       875  ...  REAL\n",
              "...     ...  ...   ...\n",
              "6330   4490  ...  REAL\n",
              "6331   8062  ...  FAKE\n",
              "6332   8622  ...  FAKE\n",
              "6333   4021  ...  REAL\n",
              "6334   4330  ...  REAL\n",
              "\n",
              "[6335 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Q8OvYC_b-n"
      },
      "source": [
        "# setup\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from assets.confint import classification_confint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "zJ4ajLf0_oPk",
        "outputId": "1ce7e8a2-ec03-4691-bda5-9c260757ff89"
      },
      "source": [
        "\n",
        "print(\"******** data **********\")\n",
        "\n",
        "# get the newsgroup database\n",
        "cats = ['talk.politics.misc', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', \n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=cats)\n",
        "\n",
        "# extract into dataframes\n",
        "texts = pd.DataFrame(newsgroups_train.data, columns=['text'])\n",
        "labels = pd.DataFrame(newsgroups_train.target, columns=['label'])['label'].apply(lambda x: cats[x])\n",
        "texts.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** data **********\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nIn billions of dollars (%GNP):\\nyear  GNP   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ajteel@dendrite.cs.Colorado.EDU (A.J. Teel) w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nMy opinion is this:  In a society whose econ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ahhh, remember the days of Yesterday?  When we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\"...a la Chrysler\"??  Okay kids, to the near...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  \\nIn billions of dollars (%GNP):\\nyear  GNP   ...\n",
              "1   ajteel@dendrite.cs.Colorado.EDU (A.J. Teel) w...\n",
              "2  \\nMy opinion is this:  In a society whose econ...\n",
              "3  Ahhh, remember the days of Yesterday?  When we...\n",
              "4  \\n\"...a la Chrysler\"??  Okay kids, to the near..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "9nLt6wT6_yFu",
        "outputId": "cd153160-9aae-4787-f28c-75ee8a2031df"
      },
      "source": [
        "texts.iloc[3,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ahhh, remember the days of Yesterday?  When we were only \\n\\tgoing to pay $17 / month?\\n\\n\\tWhen only 1.2% of the population would pay extra taxes?\\n\\n\\tRemember when a few of us predicted that it wasn\\'t true?  :)\\n\\tRemember the Inaugural?   Dancing and Singing!  Liberation\\n\\tat last!  \\n\\n\\tWell, figure *this* out:\\n\\n\\t5% VAT, estimated to raise $60-100 Billion per year ( on CNN )\\n\\tWork it out, chum...\\n\\n\\t     $60,000,000,000  /  125,000,000 taxpayers = $480 / year\\n\\n        But, you exclaim, \" I\\'ll get FREE HEALTH CARE! \"\\n\\tBut, I exclaim, \" No, you won\\'t! \"\\n\\n\\tThis is only for that poor 37 million who have none.  Not for\\n\\tYOU, chum. :)  That comes LATER.\\n\\n\\tAdd in the estimates of the energy tax costs - $300-500 / year\\n\\n\\tPlus, all that extra \"corporate and rich\" taxes that will \\n\\ttrickle down, and what do you have?\\n\\n\\t$1,000 / year, just like I said two months ago.\\n\\n\\tAnd, the best part?   You don\\'t GET ANYTHING for it.\\n\\n\\tDeficit is STILL projected to rise at same rate it\\'s  been\\n\\trising at, by CLINTON\\'S OWN ESTIMATES.  And this assumes that\\n\\this plan WILL WORK!\\n\\n\\tI mean, come on, it doesn\\'t take a ROCKET SCIENTIST to see\\n\\tthat in another 2 or 3 years, we\\'re GETTING ANOTHER WHOPPING\\n\\tTAX INCREASE, because the deficit will STILL be GROWING \\n\\tFASTER THAN the ECONOMY.\\n\\n\\tAll Clinton is doing, is moving us to a HIGHER diving board.\\n\\n        Face it.  Clinton is Bush X 2.  In four more years, our\\n\\tcountry will be completely bankrupt, and your children\\'s\\n\\tfuture, so oft mentioned by Pal Bill, will be gone.\\n\\n\\tAnd those of you still deluding yourselves will be faced\\n\\twith the guilt.\\n\\n\\tWell, <glancing at watch>, gotta go.  I want to be out of\\n\\there by noon.  Got an appointment at the lake.  No tax\\n\\tthere, yet.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0KecLw__5ok",
        "outputId": "06d2bf1d-1c0d-46fe-a149-6d4d3794de6a"
      },
      "source": [
        "labels.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    sci.space\n",
              "1    sci.space\n",
              "2    sci.space\n",
              "3    sci.space\n",
              "4    sci.space\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d4V54DN_9LQ",
        "outputId": "b58affea-93f6-4450-b3bd-f2883734754b"
      },
      "source": [
        "print(\"******** docarray **********\")\n",
        "\n",
        "# build the stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "# get the default text analyzer from CountVectorizer\n",
        "analyzer = vectorizer = CountVectorizer(analyzer = \"word\", token_pattern = \"[a-zA-Z]+\").build_analyzer()\n",
        "\n",
        "# build a new analyzer that stems using the default analyzer to create the words to be stemmed\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "# build docarrayu\n",
        "vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
        "                                 binary=True,\n",
        "                                 min_df=2)\n",
        "docarray = vectorizer.fit_transform(texts.loc[:,'text']).toarray()\n",
        "docarray.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** docarray **********\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1058, 6267)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0XOKwVfADTg",
        "outputId": "05f771c2-edef-4412-9ea1-ced91c9dff86"
      },
      "source": [
        "print(\"******** model **********\")\n",
        "\n",
        "\n",
        "# KNN\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "# grid search\n",
        "param_grid = {'n_neighbors': list(range(1,15,3))}\n",
        "grid = GridSearchCV(model, param_grid, cv=2, verbose=10, n_jobs=-1)\n",
        "grid.fit(docarray, labels)\n",
        "print(\"Grid Search: best parameters: {}\".format(grid.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** model **********\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    5.8s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   11.6s\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   28.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Grid Search: best parameters: {'n_neighbors': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWWspa__ALAM",
        "outputId": "dbca8964-5775-4645-9633-b35885c1a2f8"
      },
      "source": [
        "print(\"******** Accuracy **********\")\n",
        "\n",
        "# accuracy of best model with confidence interval\n",
        "best_model = grid.best_estimator_\n",
        "predict_y = best_model.predict(docarray)\n",
        "acc = accuracy_score(labels, predict_y)\n",
        "lb,ub = classification_confint(acc,docarray.shape[0])\n",
        "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** Accuracy **********\n",
            "Accuracy: 0.86 (0.84,0.88)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNKAoo9UAOwu",
        "outputId": "2210aba8-838e-4a50-ed4e-c045167b52b1"
      },
      "source": [
        "\n",
        "print(\"******** confusion matrix **********\")\n",
        "\n",
        "# build the confusion matrix\n",
        "cm = confusion_matrix(labels, predict_y, labels=cats)\n",
        "cm_df = pd.DataFrame(cm, index=cats, columns=cats)\n",
        "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** confusion matrix **********\n",
            "Confusion Matrix:\n",
            "                    talk.politics.misc  sci.space\n",
            "talk.politics.misc                 562         31\n",
            "sci.space                           19        446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koeIFutQAZWQ"
      },
      "source": [
        "# Naive Bayes(NB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y_VBOEVAVuY",
        "outputId": "45f19ce2-4f52-47c4-9e68-51cd90f7635d"
      },
      "source": [
        "## Naive Bayes\n",
        "\n",
        "# setup\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from assets.confint import classification_confint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "print(\"******** data **********\")\n",
        "\n",
        "# get the newsgroup database\n",
        "cats = ['talk.politics.misc', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', \n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=cats)\n",
        "\n",
        "# extract into dataframes\n",
        "texts = pd.DataFrame(newsgroups_train.data, columns=['text'])\n",
        "labels = pd.DataFrame(newsgroups_train.target, columns=['label'])['label'].apply(lambda x: cats[x])\n",
        "\n",
        "print(\"******** docarray **********\")\n",
        "\n",
        "# build the stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "# get the default text analyzer from CountVectorizer\n",
        "analyzer = vectorizer = CountVectorizer(analyzer = \"word\", token_pattern = \"[a-zA-Z]+\").build_analyzer()\n",
        "\n",
        "# build a new analyzer that stems using the default analyzer to create the words to be stemmed\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "# build docarrayu\n",
        "vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
        "                                 binary=True,\n",
        "                                 min_df=2)\n",
        "docarray = vectorizer.fit_transform(texts.loc[:,'text']).toarray()\n",
        "docarray.shape\n",
        "\n",
        "print(\"******** model **********\")\n",
        "\n",
        "\n",
        "# Naive Bayes\n",
        "model = MultinomialNB()\n",
        "# NOTE: NB does not have any hyper-parameters - no overfitting - no searching over parameter space!\n",
        "model.fit(docarray, labels)\n",
        "\n",
        "\n",
        "print(\"******** Accuracy **********\")\n",
        "\n",
        "# accuracy of best model with confidence interval\n",
        "best_model = model\n",
        "predict_y = best_model.predict(docarray)\n",
        "acc = accuracy_score(labels, predict_y)\n",
        "lb,ub = classification_confint(acc,docarray.shape[0])\n",
        "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n",
        "\n",
        "print(\"******** confusion matrix **********\")\n",
        "\n",
        "# build the confusion matrix\n",
        "cm = confusion_matrix(labels, predict_y, labels=cats)\n",
        "cm_df = pd.DataFrame(cm, index=cats, columns=cats)\n",
        "print(\"Confusion Matrix:\\n{}\".format(cm_df))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** data **********\n",
            "******** docarray **********\n",
            "******** model **********\n",
            "******** Accuracy **********\n",
            "Accuracy: 0.95 (0.94,0.97)\n",
            "******** confusion matrix **********\n",
            "Confusion Matrix:\n",
            "                    talk.politics.misc  sci.space\n",
            "talk.politics.misc                 562         31\n",
            "sci.space                           19        446\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}